\section{Reflection}
\paragraph{Reflections of test cases} was done after the execution, and we discovered several deficiencies.

We discovered that the first two test cases for the music player had a significant overlap: if the application did not locate any .mp3 files on the device (test 2), it would not be possible to play it (test 1).
It proved not to be a problem, as both tests passed, but it illustrated the importance of considering how one test influences the others.
In this case, test 2 (\textit{locate .mp3 files on device}) should have either been a precondition for test 1 (\textit{play .mp3 files}) or the two tests should have been merged into one.\\

The last test for the music player, \textit{adjust music speed}, turned out to be somewhat vaguely described, and with an unnecessary procedure.
Some other test cases have some similar problems, but as this is the best example, we will focus on the details of this particular test.

The first precondition is fine - the application has to be running.
The second says that music must be playing, which is a clear indication of the test being written after the application was developed.
One of the shortcomings of it being written so late, is that it does not specify how to start a song when the user starts running, as it was already implemented, that issue should have had its own user story and acceptance test, but that is besides the point here.
The last precondition says that the device must be on the user's arm, and the procedure says they have to start running followed by increasing the running pace.
First, there is no specified pace, which can lead to problems concerning, at what pace difference should the tempo change, and how much the tempo should increase. 
Secondly it is not specified which song(s) should be played, which can lead to problems concerning, if the song's tempo is fit to adjustment instead of song change.

Further, this whole test can easily be performed without having to run: gently moving the phone up and down works just as well, and the running pace can be read and verified directly on the screen.
The postcondition is a bit unclear, and would likely cause problems if somebody unfamiliar with the system had to do the test: should the tempo of the song already playing be modified, or should a new song with a matching tempo be playing? 
This test is supposed to test the former of the options, and another test should test the latter.

This example shows it is important to be precise in the test description, and to consider how to execute the tests as simple as possible while still providing useful results.\\

The test cases created for this project gave an overall idea of how well the application lived up to our expectations.
Most of the main features of the application were tested, but as with most types of software testing, it was hard to test for everything, so some of the less important areas have not been tested.

\paragraph{Automated acceptance tests} are useful, as the entire test suite can be run, and a complete progress overview will be generated.
This gives the customer, managers, and developers a better grasp of how the project is progressing, and what to implement next.
However, it takes very much time and effort to automate these acceptance tests.
In spite of all the advantages of automating acceptance tests, we abandoned the practice due to the huge time and effort requirements.

We looked at a behaviour-driven development framework called Robotium.
The tool was great at simulating and notice changes to the (G)UI.
Although, the tool's features covered most of the needs for our acceptance tests, the tests for our Non-graphical User Interface (NGUI) was not feasible to test with this tool, due to the need to differentiate between the sounds of two songs.
It could have been implemented, but doing so would mean to implement an entire module for sound recognition, and it was deemed too costly to be feasible. 

Through our unit test suite, we discovered that automating changes to the Android GUI is not always accepted by the system, and can sometimes cause errors.
We are not sure if this will cause problems for the Robotium tool, but it gives cause for concern in relation to time constraints.

\paragraph{Prior knowledge of the code} has affected our acceptance tests, since they were written after the code, as mentioned prior in the section. We suspect that this had the consequence of resulting in different tests, than if we did now know anything about the code. 

Because we had knowledge of the code, one could say that we had white box knowledge of the code.
We knew where its strengths and weaknesses were, so we naturally had a different insight which resulted in different acceptance tests.
This possibly had the consequence of us writing tests to pass the code, and not the other way around. 

If a customer had written the acceptance tests, they would possibly had been written differently. One major difference would be the lack of insight into the code and general knowledge about programming, which could result in a less technical acceptance test, unless supervised by a developer.
Another difference is the knowledge of the problem domain.
We are developers trying to develop an application for runners, whereas the customer is a runner developing an application for runners.
This could lead to shifting the focus of the acceptance test, to a more functional aspect, or maybe more specific functionality, which in turn would be extremely detailed. In this project it could have been more focus about the running form, where we completely disregard this, as we have almost no knowledge about it.
